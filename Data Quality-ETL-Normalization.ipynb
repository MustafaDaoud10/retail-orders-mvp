{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33f2b662",
   "metadata": {},
   "source": [
    "# Retail Orders Data — Cleaning, Validation, and Quality Report\n",
    "\n",
    "**Author:** Mustafa \n",
    "**Purpose:** Prepare monthly retail orders for analytics by:\n",
    "- Selecting **latest snapshot per period** from drop folder  \n",
    "- Loading `|`-delimited CSVs with **ANSI codepage** handling  \n",
    "- Standardizing types (dates, numerics, text)  \n",
    "- Running **data quality checks** (duplicates, logic, constraints, consistency)  \n",
    "- Exporting a professional **Quality Report** for stakeholders\n",
    "\n",
    "_Notes:_ Source files may be re-extracts for the same month. We retain only the latest snapshot to prevent double counting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a4059caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas openpyxl python-dateutil charset-normalizer\n",
    "\n",
    "import os, re, io\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from dateutil import parser as dparser\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee6e32",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We expect `|`-delimited CSVs inside `Case_Study_Data_For_Share`.  \n",
    "“ANSI” typically maps to **Windows code pages** (e.g., `cp1252` Western European, `cp1256` Arabic).  \n",
    "If unsure, we’ll **auto-detect** and fall back to a sensible default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "18fff805",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR   = \"Case_Study_Data_For_Share\"\n",
    "DELIM      = \"|\"\n",
    "\n",
    "\n",
    "DEFAULT_ANSI = \"cp1252\"\n",
    "\n",
    "EXPECTED_COLS = [\n",
    "    \"Row ID\",\"Order ID\",\"Order Date\",\"Ship Date\",\"Ship Mode\",\n",
    "    \"Customer ID\",\"Customer Name\",\"Segment\",\n",
    "    \"Country\",\"City\",\"State\",\"Postal Code\",\"Region\",\n",
    "    \"Product ID\",\"Category\",\"Sub-Category\",\"Product Name\",\n",
    "    \"Sales\",\"Quantity\",\"Discount\",\"Profit\",\"__period_yyyymm\",\"__source_file\"\n",
    "]\n",
    "\n",
    "DATE_COLS = [\"Order Date\",\"Ship Date\"]\n",
    "NUM_COLS  = [\"Sales\",\"Quantity\",\"Discount\",\"Profit\"]\n",
    "\n",
    "pd.set_option(\"display.float_format\", lambda v: f\"{v:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sniff_encoding(path, default=DEFAULT_ANSI, sample_bytes=100_000):\n",
    "\n",
    "    try:\n",
    "        import charset_normalizer as cn\n",
    "        with open(path, \"rb\") as f:\n",
    "            raw = f.read(sample_bytes)\n",
    "        res = cn.from_bytes(raw).best()\n",
    "        if res and res.encoding:\n",
    "            return res.encoding\n",
    "    except Exception:\n",
    "        pass\n",
    "    return default\n",
    "\n",
    "\n",
    "def parse_file_meta(fname):\n",
    "    \"\"\"\n",
    "    Parse period (YYYYMM) and optional snapshot timestamp from filename.\n",
    "    Examples:\n",
    "      202101_Orders_2021_02_03_12_30_05.csv\n",
    "      Orders_2023-06-01_18-22-40.csv\n",
    "    \"\"\"\n",
    "    base = os.path.basename(fname)\n",
    "\n",
    "    # period: strict YYYYMM anywhere\n",
    "    m_period = re.search(r\"(?P<yyyymm>20\\d{2}[01]\\d)\", base)\n",
    "\n",
    "    m_snap = re.search(\n",
    "        r\"(?P<y>20\\d{2})[^\\d]?(?P<m>[01]?\\d)[^\\d]?(?P<d>[0-3]?\\d)[^\\d_ -]?\"\n",
    "        r\"(?P<H>[0-2]?\\d)?[^\\d]?(?P<M>[0-5]?\\d)?[^\\d]?(?P<S>[0-5]?\\d)?\",\n",
    "        base\n",
    "    )\n",
    "\n",
    "    period = m_period.group(\"yyyymm\") if m_period else None\n",
    "    snap_dt = None\n",
    "    if m_snap:\n",
    "        y = int(m_snap.group(\"y\"))\n",
    "        m = int(m_snap.group(\"m\"))\n",
    "        d = int(m_snap.group(\"d\"))\n",
    "        H = int(m_snap.group(\"H\") or 0)\n",
    "        M = int(m_snap.group(\"M\") or 0)\n",
    "        S = int(m_snap.group(\"S\") or 0)\n",
    "        try:\n",
    "            snap_dt = datetime(y, m, d, H, M, S)\n",
    "        except ValueError:\n",
    "            snap_dt = None\n",
    "\n",
    "    return {\n",
    "        \"fname\": base,\n",
    "        \"period_yyyymm\": period,\n",
    "        \"snapshot_dt\": snap_dt\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bfde1a",
   "metadata": {},
   "source": [
    "## File Selection — Latest Snapshot per Period\n",
    "\n",
    "We index all CSVs, parse `(period, snapshot)` from the filename, and keep the **latest** file per period.\n",
    "If `snapshot` is missing, we use file **modified time** as a proxy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151a29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Selected latest snapshot per period:\n",
      " • 201901 → 201901_Orders_2019_02_04_15_41_32.csv (2025-09-18 10:34:38.942024)\n",
      " • 201902 → 201902_Orders_2019_03_04_02_26_31.csv (2025-09-18 10:34:38.948954)\n",
      " • 201903 → 201903_Orders_2019_04_10_03_28_07.csv (2025-09-18 10:34:38.973642)\n",
      " • 201904 → 201904_Orders_2019_05_05_14_49_28.csv (2025-09-18 10:34:38.986784)\n",
      " • 201905 → 201905_Orders_2019_06_13_00_44_36.csv (2025-09-18 10:34:39.021011)\n",
      " • 201906 → 201906_Orders_2019_07_05_15_36_23.csv (2025-09-18 10:34:39.037537)\n",
      " • 201907 → 201907_Orders_2019_08_07_06_17_00.csv (2025-09-18 10:34:39.057819)\n",
      " • 201908 → 201908_Orders_2019_09_01_04_02_26.csv (2025-09-18 10:34:39.058920)\n",
      " • 201909 → 201909_Orders_2019_10_14_22_06_48.csv (2025-09-18 10:34:39.101699)\n",
      " • 201910 → 201910_Orders_2019_11_01_17_08_29.csv (2025-09-18 10:34:39.109143)\n",
      " • 201911 → 201911_Orders_2019_12_02_13_58_17.csv (2019-01-01 00:00:00)\n",
      " • 201912 → 201912_Orders_2020_01_13_08_37_08.csv (2019-01-02 00:00:00)\n",
      " • 202001 → 202001_Orders_2020_02_13_22_51_02.csv (2025-09-18 10:34:39.183384)\n",
      " • 202002 → 202002_Orders_2020_03_09_07_26_55.csv (2025-09-18 10:34:39.192898)\n",
      " • 202003 → 202003_Orders_2020_04_13_20_54_35.csv (2025-09-18 10:34:39.224775)\n",
      " • 202004 → 202004_Orders_2020_05_05_09_43_03.csv (2025-09-18 10:34:39.250452)\n",
      " • 202005 → 202005_Orders_2020_06_07_02_16_45.csv (2025-09-18 10:34:39.267623)\n",
      " • 202006 → 202006_Orders_2020_07_14_03_41_14.csv (2025-09-18 10:34:39.298091)\n",
      " • 202007 → 202007_Orders_2020_08_04_11_46_44.csv (2025-09-18 10:34:39.303725)\n",
      " • 202008 → 202008_Orders_2020_09_10_15_48_10.csv (2025-09-18 10:34:39.334774)\n",
      " • 202009 → 202009_Orders_2020_10_02_13_14_41.csv (2025-09-18 10:34:39.340235)\n",
      " • 202010 → 202010_Orders_2020_11_01_06_52_44.csv (2025-09-18 10:34:39.345896)\n",
      " • 202011 → 202011_Orders_2020_12_05_00_07_22.csv (2020-01-01 00:00:00)\n",
      " • 202012 → 202012_Orders_2021_01_10_18_16_41.csv (2020-01-02 00:00:00)\n",
      " • 202101 → 202101_Orders_2021_02_10_21_44_38.csv (2025-09-18 10:34:39.405544)\n",
      " • 202102 → 202102_Orders_2021_03_10_17_46_44.csv (2025-09-18 10:34:39.429502)\n",
      " • 202103 → 202103_Orders_2021_04_07_04_27_03.csv (2025-09-18 10:34:39.451703)\n",
      " • 202104 → 202104_Orders_2021_05_02_06_20_30.csv (2025-09-18 10:34:39.456103)\n",
      " • 202105 → 202105_Orders_2021_06_01_06_40_40.csv (2025-09-18 10:34:39.465578)\n",
      " • 202106 → 202106_Orders_2021_07_11_06_56_45.csv (2025-09-18 10:34:39.492250)\n",
      " • 202107 → 202107_Orders_2021_08_04_14_33_38.csv (2025-09-18 10:34:39.507882)\n",
      " • 202108 → 202108_Orders_2021_09_10_15_19_50.csv (2025-09-18 10:34:39.540562)\n",
      " • 202109 → 202109_Orders_2021_10_08_07_34_48.csv (2025-09-18 10:34:39.562544)\n",
      " • 202110 → 202110_Orders_2021_11_04_02_06_38.csv (2025-09-18 10:34:39.576262)\n",
      " • 202111 → 202111_Orders_2021_12_11_17_08_12.csv (2021-01-01 00:00:00)\n",
      " • 202112 → 202112_Orders_2022_01_13_17_00_41.csv (2021-01-02 00:00:00)\n",
      " • 202201 → 202201_Orders_2022_02_01_20_28_12.csv (2025-09-18 10:34:39.644843)\n",
      " • 202202 → 202202_Orders_2022_03_04_03_49_38.csv (2025-09-18 10:34:39.647263)\n",
      " • 202203 → 202203_Orders_2022_04_01_04_20_24.csv (2025-09-18 10:34:39.660062)\n",
      " • 202204 → 202204_Orders_2022_05_11_17_25_10.csv (2025-09-18 10:34:39.698858)\n",
      " • 202205 → 202205_Orders_2022_06_01_22_34_23.csv (2025-09-18 10:34:39.698858)\n",
      " • 202206 → 202206_Orders_2022_07_05_18_29_34.csv (2025-09-18 10:34:39.710365)\n",
      " • 202207 → 202207_Orders_2022_08_10_16_42_17.csv (2025-09-18 10:34:39.733287)\n",
      " • 202208 → 202208_Orders_2022_09_10_12_41_11.csv (2025-09-18 10:34:39.757777)\n",
      " • 202209 → 202209_Orders_2022_10_05_12_45_07.csv (2025-09-18 10:34:39.781295)\n",
      " • 202210 → 202210_Orders_2022_11_01_00_08_55.csv (2025-09-18 10:34:39.789280)\n",
      " • 202211 → 202211_Orders_2022_12_11_08_20_07.csv (2022-01-01 00:00:00)\n",
      " • 202212 → 202212_Orders_2023_01_01_11_00_14.csv (2022-01-02 00:00:00)\n"
     ]
    }
   ],
   "source": [
    "all_files = sorted(glob(os.path.join(DATA_DIR, \"*.csv\")))\n",
    "if not all_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {DATA_DIR}\")\n",
    "\n",
    "meta_rows = []\n",
    "for path in all_files:\n",
    "    meta = parse_file_meta(path)\n",
    "    meta[\"path\"] = path\n",
    "    meta_rows.append(meta)\n",
    "\n",
    "meta_df = pd.DataFrame(meta_rows)\n",
    "\n",
    "meta_df[\"fallback_mtime\"] = meta_df[\"path\"].apply(lambda p: datetime.fromtimestamp(os.path.getmtime(p)))\n",
    "meta_df[\"effective_snapshot\"] = meta_df[\"snapshot_dt\"].fillna(meta_df[\"fallback_mtime\"])\n",
    "\n",
    "latest_per_period = (\n",
    "    meta_df[meta_df[\"period_yyyymm\"].notna()]\n",
    "    .sort_values([\"period_yyyymm\", \"effective_snapshot\"])\n",
    "    .groupby(\"period_yyyymm\", as_index=False)\n",
    "    .tail(1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Selected latest snapshot per period:\")\n",
    "for _, r in latest_per_period.iterrows():\n",
    "    print(f\" • {r['period_yyyymm']} → {r['fname']} ({r['effective_snapshot']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec2ff69",
   "metadata": {},
   "source": [
    "## Data Loading — Robust to ANSI & Quotes\n",
    "\n",
    "CSV quirks addressed:\n",
    "- `|` delimiter\n",
    "- ANSI codepage (auto-detected with fallback)\n",
    "- Irregular quoting or bad lines → we **skip** malformed lines (logged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae11fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded rows: 9,994 from 48 latest files.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>...</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "      <th>__source_file</th>\n",
       "      <th>__period_yyyymm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7981</td>\n",
       "      <td>CA-2019-103800</td>\n",
       "      <td>04-01-2019</td>\n",
       "      <td>08-01-2019</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>DP-13000</td>\n",
       "      <td>Darren Powers</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Houston</td>\n",
       "      <td>...</td>\n",
       "      <td>OFF-PA-10000174</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Paper</td>\n",
       "      <td>Message Book, Wirebound, Four 5 1/2\" X 4\" Form...</td>\n",
       "      <td>16.45</td>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5.55</td>\n",
       "      <td>201901_Orders_2019_02_04_15_41_32.csv</td>\n",
       "      <td>201901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>740</td>\n",
       "      <td>CA-2019-112326</td>\n",
       "      <td>05-01-2019</td>\n",
       "      <td>09-01-2019</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>PO-19195</td>\n",
       "      <td>Phillina Ober</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>United States</td>\n",
       "      <td>Naperville</td>\n",
       "      <td>...</td>\n",
       "      <td>OFF-LA-10003223</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Labels</td>\n",
       "      <td>Avery 508</td>\n",
       "      <td>11.78</td>\n",
       "      <td>3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4.27</td>\n",
       "      <td>201901_Orders_2019_02_04_15_41_32.csv</td>\n",
       "      <td>201901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>741</td>\n",
       "      <td>CA-2019-112326</td>\n",
       "      <td>05-01-2019</td>\n",
       "      <td>09-01-2019</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>PO-19195</td>\n",
       "      <td>Phillina Ober</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>United States</td>\n",
       "      <td>Naperville</td>\n",
       "      <td>...</td>\n",
       "      <td>OFF-ST-10002743</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>SAFCO Boltless Steel Shelving</td>\n",
       "      <td>272.74</td>\n",
       "      <td>3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-64.77</td>\n",
       "      <td>201901_Orders_2019_02_04_15_41_32.csv</td>\n",
       "      <td>201901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
       "0    7981  CA-2019-103800  04-01-2019  08-01-2019  Standard Class    DP-13000   \n",
       "1     740  CA-2019-112326  05-01-2019  09-01-2019  Standard Class    PO-19195   \n",
       "2     741  CA-2019-112326  05-01-2019  09-01-2019  Standard Class    PO-19195   \n",
       "\n",
       "   Customer Name      Segment        Country        City  ...  \\\n",
       "0  Darren Powers     Consumer  United States     Houston  ...   \n",
       "1  Phillina Ober  Home Office  United States  Naperville  ...   \n",
       "2  Phillina Ober  Home Office  United States  Naperville  ...   \n",
       "\n",
       "        Product ID         Category Sub-Category  \\\n",
       "0  OFF-PA-10000174  Office Supplies        Paper   \n",
       "1  OFF-LA-10003223  Office Supplies       Labels   \n",
       "2  OFF-ST-10002743  Office Supplies      Storage   \n",
       "\n",
       "                                        Product Name  Sales Quantity Discount  \\\n",
       "0  Message Book, Wirebound, Four 5 1/2\" X 4\" Form...  16.45        2     0.20   \n",
       "1                                          Avery 508  11.78        3     0.20   \n",
       "2                      SAFCO Boltless Steel Shelving 272.74        3     0.20   \n",
       "\n",
       "   Profit                          __source_file  __period_yyyymm  \n",
       "0    5.55  201901_Orders_2019_02_04_15_41_32.csv           201901  \n",
       "1    4.27  201901_Orders_2019_02_04_15_41_32.csv           201901  \n",
       "2  -64.77  201901_Orders_2019_02_04_15_41_32.csv           201901  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_csv_robust(path, delim=DELIM):\n",
    "    enc = sniff_encoding(path)\n",
    "    try_orders = [\n",
    "        dict(encoding=enc, engine=\"python\", on_bad_lines=\"skip\"),\n",
    "        dict(encoding=enc, engine=\"c\", on_bad_lines=\"skip\"),\n",
    "    ]\n",
    "\n",
    "    last_err = None\n",
    "    for opts in try_orders:\n",
    "        try:\n",
    "            return pd.read_csv(path, delimiter=delim, **opts)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "\n",
    "parts = []\n",
    "for _, row in latest_per_period.iterrows():\n",
    "    df_part = read_csv_robust(row[\"path\"])\n",
    "    df_part[\"__source_file\"]   = row[\"fname\"]\n",
    "    df_part[\"__period_yyyymm\"] = row[\"period_yyyymm\"]\n",
    "    parts.append(df_part)\n",
    "\n",
    "raw = pd.concat(parts, ignore_index=True)\n",
    "print(f\"Loaded rows: {len(raw):,} from {len(parts)} latest files.\")\n",
    "raw.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58c86f",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "- Keep only **expected columns** (in order)  \n",
    "- Trim text, unify whitespace  \n",
    "- Coerce numerics (`Sales`, `Quantity`, `Discount`, `Profit`)  \n",
    "- Parse dates (`Order Date`, `Ship Date`) robustly  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138042e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure expected columns exist; if extra columns, ignore\n",
    "missing = [c for c in EXPECTED_COLS if c not in raw.columns]\n",
    "if missing:\n",
    "    print(\"⚠️ Missing columns found:\", missing)\n",
    "\n",
    "df = raw.copy()\n",
    "df = df[[c for c in EXPECTED_COLS if c in df.columns]]  # keep order\n",
    "\n",
    "# Trim string columns\n",
    "for col in df.select_dtypes(include=\"object\"):\n",
    "    df[col] = df[col].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "# Numeric coercion\n",
    "for col in [c for c in NUM_COLS if c in df.columns]:\n",
    "    df[col] = (\n",
    "        df[col]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \"\", regex=False)\n",
    "        .str.replace(\" \", \"\", regex=False)\n",
    "    )\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Date parsing (tolerant to multiple formats)\n",
    "def parse_date_safe(s):\n",
    "    if pd.isna(s) or str(s).strip() == \"\":\n",
    "        return pd.NaT\n",
    "    try:\n",
    "        return dparser.parse(str(s), dayfirst=False, yearfirst=False, fuzzy=True)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "for col in [c for c in DATE_COLS]:\n",
    "    df[col] = df[col].apply(parse_date_safe).dt.normalize()\n",
    "\n",
    "\n",
    "\n",
    "if all(c in df.columns for c in [\"Order Date\",\"Ship Date\"]):\n",
    "    df[\"_DeliveryDays\"] = (df[\"Ship Date\"] - df[\"Order Date\"]).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee3f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format label counts (Order Date):\n",
      "_OrderDate_fmt\n",
      "YMD      6020\n",
      "AMBIG    3974\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Format label counts (Ship Date):\n",
      "_ShipDate_fmt\n",
      "YMD      6116\n",
      "AMBIG    3878\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Strict mismatches: 2552\n",
      "Broad mismatches : 2552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>_OrderDate_fmt</th>\n",
       "      <th>_ShipDate_fmt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA-2019-141817</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>2019-01-13</td>\n",
       "      <td>AMBIG</td>\n",
       "      <td>YMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CA-2019-105417</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>2019-01-13</td>\n",
       "      <td>AMBIG</td>\n",
       "      <td>YMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CA-2019-105417</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>2019-01-13</td>\n",
       "      <td>AMBIG</td>\n",
       "      <td>YMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CA-2019-135405</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>AMBIG</td>\n",
       "      <td>YMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CA-2019-135405</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>AMBIG</td>\n",
       "      <td>YMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>CA-2022-156720</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>YMD</td>\n",
       "      <td>AMBIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>CA-2022-143259</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>YMD</td>\n",
       "      <td>AMBIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>CA-2022-143259</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>YMD</td>\n",
       "      <td>AMBIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>CA-2022-115427</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>YMD</td>\n",
       "      <td>AMBIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>CA-2022-143259</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>YMD</td>\n",
       "      <td>AMBIG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2552 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Order ID  Order Date   Ship Date _OrderDate_fmt _ShipDate_fmt\n",
       "4     CA-2019-141817  2019-06-01  2019-01-13          AMBIG           YMD\n",
       "14    CA-2019-105417  2019-08-01  2019-01-13          AMBIG           YMD\n",
       "15    CA-2019-105417  2019-08-01  2019-01-13          AMBIG           YMD\n",
       "16    CA-2019-135405  2019-10-01  2019-01-14          AMBIG           YMD\n",
       "17    CA-2019-135405  2019-10-01  2019-01-14          AMBIG           YMD\n",
       "...              ...         ...         ...            ...           ...\n",
       "9989  CA-2022-156720  2022-12-31  2023-04-01            YMD         AMBIG\n",
       "9990  CA-2022-143259  2022-12-31  2023-04-01            YMD         AMBIG\n",
       "9991  CA-2022-143259  2022-12-31  2023-04-01            YMD         AMBIG\n",
       "9992  CA-2022-115427  2022-12-31  2023-04-01            YMD         AMBIG\n",
       "9993  CA-2022-143259  2022-12-31  2023-04-01            YMD         AMBIG\n",
       "\n",
       "[2552 rows x 5 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def _classify_date_format(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Classifies a date string (after '/' -> '-' normalization) into a token order:\n",
    "      - 'YMD'  e.g., 2024-09-19\n",
    "      - 'YDM'  e.g., 2024-19-09\n",
    "      - 'DMY'  e.g., 19-09-2024\n",
    "      - 'MDY'  e.g., 09-19-2024\n",
    "      - 'AMBIG' (cannot distinguish reliably, e.g., 05-06-2024 with both <=12)\n",
    "      - 'MISSING' / 'UNKNOWN'\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return \"MISSING\"\n",
    "    s = str(s).strip()\n",
    "    if s == \"\" or s.lower() in {\"nan\", \"nat\", \"none\"}:\n",
    "        return \"MISSING\"\n",
    "\n",
    "    parts = re.findall(r\"\\d+\", s)\n",
    "    if len(parts) != 3:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    a, b, c = parts  # a-b-c\n",
    "    # Convert safely\n",
    "    try:\n",
    "        ai, bi, ci = int(a), int(b), int(c)\n",
    "    except Exception:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    # Heuristics for yyyy-mm-dd / yyyy-dd-mm\n",
    "    if len(a) == 4:  # starts with year\n",
    "        # b could be month or day\n",
    "        b_is_month = 1 <= bi <= 12\n",
    "        b_is_day   = 1 <= bi <= 31\n",
    "        # c must be day or month\n",
    "        c_is_month = 1 <= ci <= 12\n",
    "        c_is_day   = 1 <= ci <= 31\n",
    "\n",
    "        # Prefer unambiguous:\n",
    "        if b_is_month and c_is_day and not c_is_month:\n",
    "            return \"YMD\"\n",
    "        if b_is_day and c_is_month and not b_is_month:\n",
    "            return \"YDM\"\n",
    "        # If both month/day plausible, ambiguous\n",
    "        if b_is_month and c_is_day and c_is_month:\n",
    "            return \"AMBIG\"\n",
    "        if b_is_day and c_is_month and b_is_month:\n",
    "            return \"AMBIG\"\n",
    "        # Fallback guesses\n",
    "        if b_is_month and c_is_day:\n",
    "            return \"YMD\"\n",
    "        if b_is_day and c_is_month:\n",
    "            return \"YDM\"\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    # Heuristics for dd-mm-yyyy or mm-dd-yyyy\n",
    "    if len(c) == 4:  # ends with year\n",
    "        a_is_month = 1 <= ai <= 12\n",
    "        a_is_day   = 1 <= ai <= 31\n",
    "        b_is_month = 1 <= bi <= 12\n",
    "        b_is_day   = 1 <= bi <= 31\n",
    "\n",
    "        # If unambiguous (e.g., a>12 so it's day)\n",
    "        if a_is_day and not a_is_month and b_is_month:\n",
    "            return \"DMY\"\n",
    "        # If unambiguous (e.g., b>12 so it's day)\n",
    "        if a_is_month and b_is_day and not b_is_month:\n",
    "            return \"MDY\"\n",
    "        # Both <=12 -> ambiguous (could be DMY or MDY)\n",
    "        if a_is_month and b_is_month:\n",
    "            return \"AMBIG\"\n",
    "        # Fallbacks\n",
    "        if a_is_day and b_is_month:\n",
    "            return \"DMY\"\n",
    "        if a_is_month and b_is_day:\n",
    "            return \"MDY\"\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    # If neither first nor last look like a 4-digit year\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "for col in [\"Order Date\", \"Ship Date\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = (\n",
    "            df[col]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.replace(\"/\", \"-\", regex=False)\n",
    "        )\n",
    "\n",
    "# Classify formats\n",
    "df[\"_OrderDate_fmt\"] = df[\"Order Date\"].apply(_classify_date_format) if \"Order Date\" in df.columns else \"MISSING\"\n",
    "df[\"_ShipDate_fmt\"]  = df[\"Ship Date\"].apply(_classify_date_format)  if \"Ship Date\" in df.columns  else \"MISSING\"\n",
    "\n",
    "# Two flags:\n",
    "# 1) Strict mismatch: both present (not MISSING/UNKNOWN) and different\n",
    "df[\"_DateFmtMismatch_strict\"] = (\n",
    "    (df[\"_OrderDate_fmt\"].isin([\"MISSING\",\"UNKNOWN\"]) == False) &\n",
    "    (df[\"_ShipDate_fmt\"].isin([\"MISSING\",\"UNKNOWN\"]) == False) &\n",
    "    (df[\"_OrderDate_fmt\"] != df[\"_ShipDate_fmt\"])\n",
    ")\n",
    "\n",
    "# 2) Broad mismatch: different OR one side missing/unknown (useful for quality scans)\n",
    "df[\"_DateFmtMismatch_broad\"] = (\n",
    "    (df[\"_OrderDate_fmt\"] != df[\"_ShipDate_fmt\"]) |\n",
    "    (df[\"_OrderDate_fmt\"].isin([\"MISSING\",\"UNKNOWN\"])) |\n",
    "    (df[\"_ShipDate_fmt\"].isin([\"MISSING\",\"UNKNOWN\"]))\n",
    ")\n",
    "\n",
    "# Quick overview\n",
    "print(\"Format label counts (Order Date):\")\n",
    "print(df[\"_OrderDate_fmt\"].value_counts(dropna=False))\n",
    "print(\"\\nFormat label counts (Ship Date):\")\n",
    "print(df[\"_ShipDate_fmt\"].value_counts(dropna=False))\n",
    "print(\"\\nStrict mismatches:\", int(df[\"_DateFmtMismatch_strict\"].sum()))\n",
    "print(\"Broad mismatches :\", int(df[\"_DateFmtMismatch_broad\"].sum()))\n",
    "\n",
    "mismatch_sample = df[df[\"_DateFmtMismatch_strict\"]][\n",
    "    [\"Order ID\",\"Order Date\",\"Ship Date\",\"_OrderDate_fmt\",\"_ShipDate_fmt\"]\n",
    "]\n",
    "mismatch_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5d95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order Date — AMBIG resolved: 3974\n",
      "Order Date — unparsed (NaT): 0\n",
      "Ship Date  — AMBIG resolved: 3878\n",
      "Ship Date  — unparsed (NaT): 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for col in [\"Order Date\", \"Ship Date\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = (\n",
    "            df[col].astype(str).str.strip().str.replace(\"/\", \"-\", regex=False)\n",
    "        )\n",
    "\n",
    "def derive_period_from_fname(fname: str):\n",
    "    if not fname or not isinstance(fname, str):\n",
    "        return np.nan\n",
    "    m = re.match(r\"^(20\\d{2}[01]\\d)\", os.path.basename(fname))  # leading yyyymm\n",
    "    return int(m.group(1)) if m else np.nan\n",
    "\n",
    "if \"__period_yyyymm\" not in df.columns:\n",
    "    if \"__source_file\" in df.columns:\n",
    "        df[\"__period_yyyymm\"] = df[\"__source_file\"].apply(derive_period_from_fname)\n",
    "    else:\n",
    "        raise AssertionError(\"Missing __period_yyyymm and __source_file; cannot infer period from filename.\")\n",
    "\n",
    "# Format classifier\n",
    "def classify_date_format(s: str) -> str:\n",
    "    if s is None: return \"MISSING\"\n",
    "    s = str(s).strip()\n",
    "    if s == \"\" or s.lower() in {\"nan\",\"nat\",\"none\"}: return \"MISSING\"\n",
    "    parts = re.findall(r\"\\d+\", s)\n",
    "    if len(parts) != 3: return \"UNKNOWN\"\n",
    "    a,b,c = parts\n",
    "\n",
    "    # yyyy-??-??\n",
    "    if len(a) == 4:\n",
    "        bi,ci = int(b), int(c)\n",
    "        b_m, c_m = 1<=bi<=12, 1<=ci<=12\n",
    "        b_d, c_d = 1<=bi<=31, 1<=ci<=31\n",
    "        if b_m and c_d and not c_m: return \"YMD\"\n",
    "        if b_d and c_m and not b_m: return \"YDM\"\n",
    "        if (b_m and c_m) or (b_d and c_d): return \"AMBIG\"\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    # ??-??-yyyy\n",
    "    if len(c) == 4:\n",
    "        ai,bi = int(a), int(b)\n",
    "        a_m, b_m = 1<=ai<=12, 1<=bi<=12\n",
    "        a_d, b_d = 1<=ai<=31, 1<=bi<=31\n",
    "        if a_d and not a_m and b_m: return \"DMY\"\n",
    "        if a_m and b_d and not b_m: return \"MDY\"\n",
    "        if a_m and b_m: return \"AMBIG\"\n",
    "        if a_d and b_m: return \"DMY\"\n",
    "        if a_m and b_d: return \"MDY\"\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "# 3) Helpers\n",
    "FORMAT_MAP = {\n",
    "    \"YMD\": \"%Y-%m-%d\",\n",
    "    \"YDM\": \"%Y-%d-%m\",\n",
    "    \"DMY\": \"%d-%m-%Y\",\n",
    "    \"MDY\": \"%m-%d-%Y\",\n",
    "}\n",
    "\n",
    "def parse_by_fmt(val, fmt_label):\n",
    "    if pd.isna(val) or fmt_label not in FORMAT_MAP: return pd.NaT\n",
    "    return pd.to_datetime(val, format=FORMAT_MAP[fmt_label], errors=\"coerce\")\n",
    "\n",
    "def _coerce_ts(y,m,d):\n",
    "    try: return pd.Timestamp(int(y), int(m), int(d))\n",
    "    except Exception: return pd.NaT\n",
    "\n",
    "# Order date ambiguity: use period month from filename\n",
    "def resolve_ambiguous_order_date(date_str, period_yyyymm):\n",
    "    if pd.isna(date_str) or pd.isna(period_yyyymm): return pd.NaT\n",
    "    s = str(date_str).strip()\n",
    "    parts = re.findall(r\"\\d+\", s)\n",
    "    if len(parts) != 3: return pd.NaT\n",
    "\n",
    "    p = int(period_yyyymm)\n",
    "    p_year, p_month = divmod(p, 100)\n",
    "    a,b,c = parts\n",
    "\n",
    "    # yyyy-??-??\n",
    "    if len(a) == 4:\n",
    "        y = int(a); bi,ci = int(b), int(c)\n",
    "        if 1<=bi<=12 and 1<=ci<=12:\n",
    "            if bi == p_month: return _coerce_ts(y, bi, ci)   # Y-M-D\n",
    "            if ci == p_month: return _coerce_ts(y, ci, bi)   # Y-D-M\n",
    "        cand_ymd = _coerce_ts(y, bi, ci)\n",
    "        cand_ydm = _coerce_ts(y, ci, bi)\n",
    "        if pd.notna(cand_ymd) and cand_ymd.month == p_month: return cand_ymd\n",
    "        if pd.notna(cand_ydm) and cand_ydm.month == p_month: return cand_ydm\n",
    "        return pd.NaT\n",
    "\n",
    "    # ??-??-yyyy\n",
    "    if len(c) == 4:\n",
    "        y = int(c); ai,bi = int(a), int(b)\n",
    "        if 1<=ai<=12 and 1<=bi<=12:\n",
    "            if ai == p_month: return _coerce_ts(y, ai, bi)   # MDY\n",
    "            if bi == p_month: return _coerce_ts(y, bi, ai)   # DMY\n",
    "        if ai>12 and 1<=bi<=12:\n",
    "            return _coerce_ts(y, bi, ai) if bi == p_month else pd.NaT\n",
    "        if bi>12 and 1<=ai<=12:\n",
    "            return _coerce_ts(y, ai, bi) if ai == p_month else pd.NaT\n",
    "        return pd.NaT\n",
    "\n",
    "    return pd.NaT\n",
    "\n",
    "# Ship date ambiguity: choose candidate closest to order date\n",
    "def ship_candidates_from_ambiguous(s):\n",
    "    if s is None: return []\n",
    "    s = str(s).strip()\n",
    "    parts = re.findall(r\"\\d+\", s)\n",
    "    if len(parts) != 3: return []\n",
    "    a,b,c = parts\n",
    "    cands = []\n",
    "    if len(a) == 4:      # yyyy-??-??\n",
    "        y,bi,ci = int(a), int(b), int(c)\n",
    "        cands += [_coerce_ts(y, bi, ci), _coerce_ts(y, ci, bi)]  # YMD vs YDM\n",
    "    elif len(c) == 4:    # ??-??-yyyy\n",
    "        y,ai,bi = int(c), int(a), int(b)\n",
    "        cands += [_coerce_ts(y, ai, bi), _coerce_ts(y, bi, ai)]  # MDY vs DMY\n",
    "    return [ts for ts in cands if pd.notna(ts)]\n",
    "\n",
    "def choose_closest_to_order(order_dt, candidates):\n",
    "    if pd.isna(order_dt) or not candidates:\n",
    "        return pd.NaT\n",
    "    diffs = [abs((cand - order_dt).days) for cand in candidates]\n",
    "    min_diff = min(diffs)\n",
    "    best_idxs = [i for i,d in enumerate(diffs) if d == min_diff]\n",
    "    if len(best_idxs) == 1:\n",
    "        return candidates[best_idxs[0]]\n",
    "    # tie-breaker: prefer >= order_dt (non-negative delivery days)\n",
    "    nn = [i for i in best_idxs if (candidates[i] - order_dt).days >= 0]\n",
    "    return candidates[nn[0]] if nn else candidates[best_idxs[0]]\n",
    "\n",
    "# ORDER DATE: classify, parse, resolve AMBIG, overwrite\n",
    "df[\"_OrderDate_fmt\"] = df[\"Order Date\"].apply(classify_date_format)\n",
    "\n",
    "df[\"_OrderDate_dt\"] = pd.NaT\n",
    "for lbl in [\"YMD\",\"YDM\",\"DMY\",\"MDY\"]:\n",
    "    m = df[\"_OrderDate_fmt\"].eq(lbl)\n",
    "    if m.any():\n",
    "        df.loc[m, \"_OrderDate_dt\"] = pd.to_datetime(df.loc[m, \"Order Date\"], format=FORMAT_MAP[lbl], errors=\"coerce\")\n",
    "\n",
    "# resolve ambiguous using period\n",
    "ambig_order_mask = df[\"_OrderDate_fmt\"].eq(\"AMBIG\") & df[\"_OrderDate_dt\"].isna()\n",
    "if ambig_order_mask.any():\n",
    "    df.loc[ambig_order_mask, \"_OrderDate_dt\"] = df.loc[ambig_order_mask, [\"Order Date\",\"__period_yyyymm\"]].apply(\n",
    "        lambda r: resolve_ambiguous_order_date(r[\"Order Date\"], r[\"__period_yyyymm\"]), axis=1\n",
    "    )\n",
    "\n",
    "# overwrite original column in ISO; keep helper\n",
    "df[\"Order Date\"] = df[\"_OrderDate_dt\"].dt.strftime(\"%Y-%m-%d\")\n",
    "df[\"_OrderDate_ambig_resolved\"] = ambig_order_mask & df[\"_OrderDate_dt\"].notna()\n",
    "df[\"_OrderDate_unparsed\"] = df[\"_OrderDate_dt\"].isna()\n",
    "\n",
    "# SHIP DATE: classify, parse deterministic, resolve AMBIG by proximity\n",
    "df[\"_ShipDate_fmt\"] = df[\"Ship Date\"].apply(classify_date_format)\n",
    "\n",
    "df[\"_ShipDate_dt\"] = pd.NaT\n",
    "# parse deterministic formats first\n",
    "for lbl in [\"YMD\",\"YDM\",\"DMY\",\"MDY\"]:\n",
    "    m = df[\"_ShipDate_fmt\"].eq(lbl)\n",
    "    if m.any():\n",
    "        df.loc[m, \"_ShipDate_dt\"] = pd.to_datetime(df.loc[m, \"Ship Date\"], format=FORMAT_MAP[lbl], errors=\"coerce\")\n",
    "\n",
    "# resolve ambiguous by picking candidate closest to Order Date\n",
    "ambig_ship_mask = df[\"_ShipDate_fmt\"].eq(\"AMBIG\") & df[\"_ShipDate_dt\"].isna()\n",
    "if ambig_ship_mask.any():\n",
    "    df.loc[ambig_ship_mask, \"_ShipDate_dt\"] = df.loc[ambig_ship_mask, [\"Ship Date\",\"_OrderDate_dt\"]].apply(\n",
    "        lambda r: choose_closest_to_order(r[\"_OrderDate_dt\"], ship_candidates_from_ambiguous(r[\"Ship Date\"])),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# overwrite original Ship Date in ISO\n",
    "df[\"Ship Date\"] = df[\"_ShipDate_dt\"].dt.strftime(\"%Y-%m-%d\")\n",
    "df[\"_ShipDate_ambig_resolved\"] = ambig_ship_mask & df[\"_ShipDate_dt\"].notna()\n",
    "df[\"_ShipDate_unparsed\"] = df[\"_ShipDate_dt\"].isna()\n",
    "\n",
    "# 6) Quick summary\n",
    "print(\"Order Date — AMBIG resolved:\", int(df[\"_OrderDate_ambig_resolved\"].sum()))\n",
    "print(\"Order Date — unparsed (NaT):\", int(df[\"_OrderDate_unparsed\"].sum()))\n",
    "print(\"Ship Date  — AMBIG resolved:\", int(df[\"_ShipDate_ambig_resolved\"].sum()))\n",
    "print(\"Ship Date  — unparsed (NaT):\", int(df[\"_ShipDate_unparsed\"].sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fc15af",
   "metadata": {},
   "source": [
    "## Data Quality Checks\n",
    "\n",
    "We produce dedicated tables for:\n",
    "- **Duplicates**:\n",
    "  - `Order ID + Product ID` duplicates  \n",
    "  - `Order ID` with different `Order Date`\n",
    "- **Logic**:\n",
    "  - `Ship Date` earlier than `Order Date`\n",
    "  - `Same Day` shipments where dates differ\n",
    "- **Ranges**:\n",
    "  - `Sales <= 0`, `Quantity == 0`, `Discount < 0 or > 1`, `Profit < 0`\n",
    "- **Consistency**:\n",
    "  - `Product ID → {Product Name, Category, Sub-Category}` stable mapping  \n",
    "  - `Customer ID → {Customer Name, Segment}` stable mapping\n",
    "  - Geography note: multiple cities can share a postal code; we **don’t** enforce one-to-one here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f97042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Quality checks complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Check</th>\n",
       "      <th>Rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Issues_ProfitOnly</td>\n",
       "      <td>2058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Consistency_Product</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consistency_Customer</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logic_SameDay_DatesNotEqual</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Duplicates_OrderProduct</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Issues_Other</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logic_ShipEarlierThanOrder</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Duplicates_OrderID_DifferentOrderDate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Check  Rows\n",
       "0                      Issues_ProfitOnly  2058\n",
       "1                    Consistency_Product   176\n",
       "2                   Consistency_Customer    45\n",
       "3            Logic_SameDay_DatesNotEqual    24\n",
       "4                Duplicates_OrderProduct    16\n",
       "5                           Issues_Other     4\n",
       "6             Logic_ShipEarlierThanOrder     0\n",
       "7  Duplicates_OrderID_DifferentOrderDate     0"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1) Duplicates: Order ID + Product ID\n",
    "if all(c in df.columns for c in [\"Order ID\",\"Product ID\"]):\n",
    "    dup_order_product = (\n",
    "        df[df.duplicated([\"Order ID\",\"Product ID\"], keep=False)]\n",
    "        .sort_values([\"Order ID\",\"Product ID\"])\n",
    "    )\n",
    "    issues[\"Duplicates_OrderProduct\"] = dup_order_product\n",
    "\n",
    "# 2) Order ID with different Order Date\n",
    "if all(c in df.columns for c in [\"Order ID\",\"Order Date\"]):\n",
    "    g = df.groupby(\"Order ID\")[\"Order Date\"].nunique()\n",
    "    orderid_diffdate = df[df[\"Order ID\"].isin(g[g > 1].index)].sort_values([\"Order ID\",\"Order Date\"])\n",
    "    issues[\"Duplicates_OrderID_DifferentOrderDate\"] = orderid_diffdate\n",
    "\n",
    "# 3) Logic: Ship Date earlier than Order Date\n",
    "if all(c in df.columns for c in [\"Order Date\",\"Ship Date\"]):\n",
    "    logic_ship_earlier = df[(df[\"Order Date\"].notna()) & (df[\"Ship Date\"].notna()) & (df[\"Ship Date\"] < df[\"Order Date\"])]\n",
    "    issues[\"Logic_ShipEarlierThanOrder\"] = logic_ship_earlier\n",
    "\n",
    "# 4) Same Day mismatch\n",
    "if all(c in df.columns for c in [\"Ship Mode\",\"Order Date\",\"Ship Date\"]):\n",
    "    same_day_mismatch = df[(df[\"Ship Mode\"].str.lower()==\"same day\") & (df[\"Order Date\"] != df[\"Ship Date\"])]\n",
    "    issues[\"Logic_SameDay_DatesNotEqual\"] = same_day_mismatch\n",
    "\n",
    "# 5) Ranges: Sales, Quantity, Discount, Profit\n",
    "issues_other = []\n",
    "if \"Sales\" in df.columns:\n",
    "    issues_other.append(df[df[\"Sales\"] <= 0].assign(_Issue=\"Sales <= 0\"))\n",
    "if \"Quantity\" in df.columns:\n",
    "    issues_other.append(df[df[\"Quantity\"] <= 0].assign(_Issue=\"Quantity == 0\"))\n",
    "if \"Discount\" in df.columns:\n",
    "    issues_other.append(df[(df[\"Discount\"] < 0) | (df[\"Discount\"] >= 1)].assign(_Issue=\"Discount out of [0,1]\"))\n",
    "issues[\"Issues_Other\"] = pd.concat(issues_other, ignore_index=True) if issues_other else pd.DataFrame()\n",
    "\n",
    "# Profit-only table (separate per your spec)\n",
    "if \"Profit\" in df.columns:\n",
    "    issues[\"Issues_ProfitOnly\"] = df[df[\"Profit\"].lt(0) | df[\"Profit\"].isna()].copy()\n",
    "\n",
    "# 6) Consistency checks — product mapping\n",
    "def inconsistent_mapping(df, key_col, attrs):\n",
    "    g = df.groupby(key_col)[attrs].nunique(dropna=False)\n",
    "    mask = np.any(g.values > 1, axis=1)\n",
    "    bad_keys = g[mask].index\n",
    "    if len(bad_keys) == 0:\n",
    "        return pd.DataFrame()\n",
    "    return df[df[key_col].isin(bad_keys)].sort_values([key_col] + attrs)\n",
    "\n",
    "if all(c in df.columns for c in [\"Product ID\",\"Product Name\",\"Category\",\"Sub-Category\"]):\n",
    "    issues[\"Consistency_Product\"] = inconsistent_mapping(\n",
    "        df, \"Product ID\", [\"Product Name\",\"Category\",\"Sub-Category\"]\n",
    "    )\n",
    "\n",
    "# 7) Consistency checks — customer mapping\n",
    "if all(c in df.columns for c in [\"Customer ID\",\"Customer Name\",\"Segment\"]):\n",
    "    issues[\"Consistency_Customer\"] = inconsistent_mapping(\n",
    "        df, \"Customer ID\", [\"Customer Name\",\"Segment\"]\n",
    "    )\n",
    "\n",
    "summary_rows = []\n",
    "for name, tbl in issues.items():\n",
    "    summary_rows.append({\"Check\": name, \"Rows\": 0 if tbl is None else len(tbl)})\n",
    "quality_summary = pd.DataFrame(summary_rows).sort_values(\"Rows\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Quality checks complete.\")\n",
    "quality_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2ab6a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_issues_df = pd.concat(\n",
    "    [df.assign(IssueType=key) for key, df in issues.items()],\n",
    "    ignore_index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "dda794e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_issues_df.to_csv(\"x.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76a1c0",
   "metadata": {},
   "source": [
    "## Quality Summary\n",
    "\n",
    "The code below fixes/removes some malformed rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5d6e983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates([\"Order ID\",\"Product ID\"], keep=\"first\",inplace=True)\n",
    "df = df[~df[\"Quantity\"]<=0]\n",
    "df = df[~df[\"Profit\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d7b4e",
   "metadata": {},
   "source": [
    "## Export — Quality Report (Excel)\n",
    "\n",
    "We create a multi-sheet Excel file using **XlsxWriter**:\n",
    "\n",
    "- **Overview** — counts by issue type  \n",
    "- **Sample** — first N rows of the standardized dataset  \n",
    "- Individual sheets for each issue table  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b9177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported Quality Report → Quality_Report.xlsx\n"
     ]
    }
   ],
   "source": [
    "report_path = os.path.join( \"Quality_Report.xlsx\")\n",
    "\n",
    "with pd.ExcelWriter(report_path, engine=\"xlsxwriter\", datetime_format=\"yyyy-mm-dd\") as xw:\n",
    "    # Overview\n",
    "    quality_summary.to_excel(xw, sheet_name=\"Overview\", index=False)\n",
    "\n",
    "    # Sample (keeps the doc readable)\n",
    "    df.head(200).to_excel(xw, sheet_name=\"Data_Sample\", index=False)\n",
    "\n",
    "    # Issues\n",
    "    for name, tbl in issues.items():\n",
    "        if tbl is None or len(tbl) == 0:\n",
    "            # write an empty table with a note\n",
    "            pd.DataFrame({\"Note\":[f\"No rows for {name}\"]}).to_excel(xw, sheet_name=name[:31], index=False)\n",
    "        else:\n",
    "            tbl.to_excel(xw, sheet_name=name[:31], index=False)\n",
    "\n",
    "    # Aesthetics: autofit columns\n",
    "    for sheet_name, worksheet in xw.sheets.items():\n",
    "        # best-effort autofit: look at DataFrame in writer book\n",
    "        try:\n",
    "            df_sheet = (\n",
    "                quality_summary if sheet_name==\"Overview\"\n",
    "                else df.head(200) if sheet_name==\"Data_Sample\"\n",
    "                else issues.get(sheet_name, None)\n",
    "            )\n",
    "            if isinstance(df_sheet, pd.DataFrame) and not df_sheet.empty:\n",
    "                for i, col in enumerate(df_sheet.columns):\n",
    "                    maxw = max(10, min(60, df_sheet[col].astype(str).str.len().max() + 2))\n",
    "                    worksheet.set_column(i, i, int(maxw))\n",
    "            else:\n",
    "                worksheet.set_column(0, 5, 24)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(f\"  Exported Quality Report → {report_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b0def",
   "metadata": {},
   "source": [
    "## Dim/Fact Prep\n",
    "\n",
    "If you’re generating star-schema tables, here’s a clean starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c975908c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims/Facts ready — dim_product:1,885, dim_customer:793, fact_orders:9,748\n"
     ]
    }
   ],
   "source": [
    "# Product Dim (dedupe by Product ID)\n",
    "dim_product = (\n",
    "    df[[\"Product ID\",\"Product Name\",\"Category\",\"Sub-Category\"]]\n",
    "    .dropna(subset=[\"Product ID\"])\n",
    "    .drop_duplicates(\"Product ID\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "dim_product[\"product_sk\"] = pd.factorize(dim_product[\"Product ID\"])[0] + 1\n",
    "\n",
    "#Geography Dim\n",
    "geo_cols = [\"Country\",\"State\",\"City\",\"Postal Code\",\"Region\"]\n",
    "present_geo = [c for c in geo_cols if c in df.columns]\n",
    "\n",
    "dim_geography = (\n",
    "    df[present_geo]\n",
    "    .dropna(how=\"all\")\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Natural key string for stable factorization\n",
    "dim_geography[\"_geo_nk\"] = dim_geography.astype(str).agg(\"|\".join, axis=1)\n",
    "dim_geography[\"geography_sk\"] = pd.factorize(dim_geography[\"_geo_nk\"])[0] + 1\n",
    "dim_geography.drop(columns=[\"_geo_nk\"], inplace=True)\n",
    "\n",
    "#Ship Mode Dim\n",
    "dim_shipmode = (\n",
    "        df[[\"Ship Mode\"]]\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "        .rename(columns={\"Ship Mode\":\"ship_mode\"})\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "dim_shipmode[\"ship_mode_sk\"] = pd.factorize(dim_shipmode[\"ship_mode\"])[0] + 1\n",
    "\n",
    "# Customer Dim\n",
    "dim_customer = (\n",
    "    df[[\"Customer ID\",\"Customer Name\",\"Segment\"]]\n",
    "    .dropna(subset=[\"Customer ID\"])\n",
    "    .drop_duplicates(\"Customer ID\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "dim_customer[\"customer_sk\"] = pd.factorize(dim_customer[\"Customer ID\"])[0] + 1\n",
    "\n",
    "\n",
    "def build_dim_date(dates: pd.Series) -> pd.DataFrame:\n",
    "    dmin, dmax = pd.to_datetime(dates.min()), pd.to_datetime(dates.max())\n",
    "    idx = pd.date_range(dmin, dmax, freq=\"D\")\n",
    "    dim = pd.DataFrame({\"date\": idx})\n",
    "    dim[\"date_sk\"] = dim[\"date\"].dt.strftime(\"%Y%m%d\").astype(int)\n",
    "    dim[\"day\"] = dim[\"date\"].dt.day\n",
    "    dim[\"month\"] = dim[\"date\"].dt.month\n",
    "    dim[\"month_name\"] = dim[\"date\"].dt.month_name()\n",
    "    dim[\"quarter\"] = dim[\"date\"].dt.quarter\n",
    "    dim[\"year\"] = dim[\"date\"].dt.year\n",
    "    dim[\"dow\"] = dim[\"date\"].dt.dayofweek\n",
    "    dim[\"is_weekend\"] = dim[\"dow\"].isin([5,6]).astype(int)\n",
    "    return dim[[\"date_sk\",\"date\",\"day\",\"month\",\"month_name\",\"quarter\",\"year\",\"dow\",\"is_weekend\"]]\n",
    "\n",
    "all_dates = pd.concat([df[\"Order Date\"].dropna(), df[\"Ship Date\"].dropna()])\n",
    "dim_date = build_dim_date(all_dates).sort_values(\"date_sk\").reset_index(drop=True)\n",
    "\n",
    "# convenient lookups\n",
    "orderdate_map = dim_date.set_index(\"date\")[\"date_sk\"]\n",
    "shipdate_map  = dim_date.set_index(\"date\")[\"date_sk\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Dims/Facts ready — dim_product:{len(dim_product):,}, \"\n",
    "    f\"dim_customer:{len(dim_customer):,}, fact_orders:{len(fact_orders):,}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6a46d8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_orders rows: 9,748\n"
     ]
    }
   ],
   "source": [
    "fact = df.copy()\n",
    "\n",
    "# Map Date SKs\n",
    "fact = fact.merge(dim_date[[\"date\",\"date_sk\"]], left_on=\"_OrderDate_dt\", right_on=\"date\", how=\"left\").rename(columns={\"date_sk\":\"order_date_sk\"}).drop(columns=[\"date\"])\n",
    "fact = fact.merge(dim_date[[\"date\",\"date_sk\"]], left_on=\"_ShipDate_dt\",  right_on=\"date\", how=\"left\").rename(columns={\"date_sk\":\"ship_date_sk\"}).drop(columns=[\"date\"])\n",
    "\n",
    "# Map Product SK\n",
    "fact = fact.merge(dim_product[[\"Product ID\",\"product_sk\"]], on=\"Product ID\", how=\"left\")\n",
    "\n",
    "# Map Customer SK\n",
    "fact = fact.merge(dim_customer[[\"Customer ID\",\"customer_sk\"]], on=\"Customer ID\", how=\"left\")\n",
    "\n",
    "# Map Geography SK (join on all present geo columns)\n",
    "if not dim_geography.empty:\n",
    "    fact = fact.merge(dim_geography.assign(_join_key=1), how=\"left\", on=[c for c in dim_geography.columns if c in geo_cols])\n",
    "\n",
    "# Map Ship Mode SK\n",
    "if not dim_shipmode.empty:\n",
    "    fact = fact.merge(dim_shipmode, left_on=\"Ship Mode\", right_on=\"ship_mode\", how=\"left\").drop(columns=[\"ship_mode\"])\n",
    "\n",
    "\n",
    "\n",
    "# Keep only needed analytics columns + degenerate Order ID\n",
    "fact_orders = fact[[\n",
    "    # Keys\n",
    "    \"order_date_sk\",\"ship_date_sk\",\"product_sk\",\"customer_sk\",\"geography_sk\",\n",
    "    \"ship_mode_sk\",\n",
    "    # Degenerate\n",
    "    \"Order ID\",\n",
    "    # Measures\n",
    "    \"Sales\",\"Quantity\",\"Discount\",\"Profit\"\n",
    "]].copy()\n",
    "\n",
    "# Drop the None column if segment_sk not present\n",
    "fact_orders = fact_orders[[c for c in fact_orders.columns if c is not None]]\n",
    "\n",
    "# Basic QC\n",
    "print(f\"fact_orders rows: {len(fact_orders):,}\")\n",
    "for k in [\"order_date_sk\",\"product_sk\",\"customer_sk\"]:\n",
    "    if k in fact_orders.columns:\n",
    "        nulls = fact_orders[k].isna().sum()\n",
    "        if nulls:\n",
    "            print(f\"⚠️  {k}: {nulls:,} nulls (check source mappings)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5fb6c625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Task_6_1_Data_Marts.zip and Task_6_2_Data_Marts_Rows.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "out_dir = Path(\"Task_6_Data_Marts\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "dim_date.to_csv(out_dir/\"dim_date.csv\", index=False)\n",
    "dim_shipmode.to_csv(out_dir/\"dim_ship_mode.csv\", index=False)\n",
    "dim_customer.to_csv(out_dir/\"dim_customer.csv\", index=False)\n",
    "dim_product.to_csv(out_dir/\"dim_product.csv\", index=False)\n",
    "dim_geography.to_csv(out_dir/\"dim_geography.csv\", index=False)\n",
    "fact_orders.to_csv(out_dir/\"fact_order_lines.csv\", index=False)\n",
    "\n",
    "# Counts summary CSV\n",
    "def counts_summary(name, df, pk_col):\n",
    "    d = {\n",
    "        \"Data Mart System Name\": name,\n",
    "        \"Count Rows\": len(df),\n",
    "        \"Count Distinct Primary Key\": df[pk_col].nunique() if pk_col in df.columns else np.nan\n",
    "    }\n",
    "    # Special rule: if a table has \"row_id\" also add distinct count of that field\n",
    "    if \"row_id\" in df.columns:\n",
    "        d[\"Count Distinct Row ID\"] = df[\"row_id\"].nunique()\n",
    "    return d\n",
    "\n",
    "rows = []\n",
    "rows.append(counts_summary(\"dim_date\", dim_date, \"date_sk\"))\n",
    "rows.append(counts_summary(\"dim_ship_mode\", dim_shipmode, \"ship_mode_sk\"))\n",
    "rows.append(counts_summary(\"dim_customer\", dim_customer, \"customer_sk\"))\n",
    "rows.append(counts_summary(\"dim_product\", dim_product, \"product_sk\"))\n",
    "rows.append(counts_summary(\"dim_geography\", dim_geography, \"geography_sk\"))\n",
    "rows.append(counts_summary(\"fact_order_lines\", fact_orders, \"order_id\"))  # natural key as PK proxy\n",
    "\n",
    "pd.DataFrame(rows).to_csv(\"Task_6_2_Data_Marts_Rows.csv\", index=False)\n",
    "\n",
    "# Zip archive\n",
    "with zipfile.ZipFile(\"Task_6_1_Data_Marts.zip\", \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "    for p in out_dir.glob(\"*.csv\"):\n",
    "        zf.write(p, arcname=p.name)\n",
    "\n",
    "print(\"Exported Task_6_1_Data_Marts.zip and Task_6_2_Data_Marts_Rows.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
